--------创建日志文件-----------2022-08-29 13:32:34 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: scrapy_modules)
2022-08-29 13:32:34 [scrapy.utils.log] INFO: Versions: lxml 4.9.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 2.0.1, Twisted 22.4.0, Python 3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 3.0.4 21 Jun 2022), cryptography 37.0.3, Platform Windows-10-10.0.19041-SP0
2022-08-29 13:32:34 [scrapy.crawler] INFO: Overridden settings:
{'BOT_NAME': 'scrapy_modules',
 'LOG_FILE': 'scrapy_2022_8_29.log',
 'NEWSPIDER_MODULE': 'scrapy_modules.spiders',
 'ROBOTSTXT_OBEY': True,
 'SPIDER_MODULES': ['scrapy_modules.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/95.0.4638.69 Safari/537.36'}
2022-08-29 13:32:34 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor
2022-08-29 13:32:34 [scrapy.extensions.telnet] INFO: Telnet Password: 25199569952695bf
2022-08-29 13:32:34 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.logstats.LogStats']
2022-08-29 13:32:35 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://localhost:56376/session {"capabilities": {"firstMatch": [{}], "alwaysMatch": {"browserName": "chrome", "pageLoadStrategy": "normal", "goog:chromeOptions": {"extensions": [], "args": []}}}}
2022-08-29 13:32:35 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:56376
2022-08-29 13:32:38 [urllib3.connectionpool] DEBUG: http://localhost:56376 "POST /session HTTP/1.1" 200 720
2022-08-29 13:32:38 [selenium.webdriver.remote.remote_connection] DEBUG: Remote response: status=200 | data={"value":{"capabilities":{"acceptInsecureCerts":false,"browserName":"chrome","browserVersion":"83.0.4103.106","chrome":{"chromedriverVersion":"83.0.4103.39 (ccbf011cb2d2b19b506d844400483861342c20cd-refs/branch-heads/4103@{#416})","userDataDir":"C:\\Users\\wuyiping\\AppData\\Local\\Temp\\scoped_dir36356_121135794"},"goog:chromeOptions":{"debuggerAddress":"localhost:56383"},"networkConnectionEnabled":false,"pageLoadStrategy":"normal","platformName":"windows","proxy":{},"setWindowRect":true,"strictFileInteractability":false,"timeouts":{"implicit":0,"pageLoad":300000,"script":30000},"unhandledPromptBehavior":"dismiss and notify","webauthn:virtualAuthenticators":true},"sessionId":"df9f3fc0700790449d6ddc8eab7484e8"}} | headers=HTTPHeaderDict({'Content-Length': '720', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})
2022-08-29 13:32:38 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request
2022-08-29 13:32:40 [twisted] CRITICAL: Unhandled error in Deferred:
2022-08-29 13:32:40 [twisted] CRITICAL: 
Traceback (most recent call last):
  File "D:\servers\anaconda3\envs\py3.7\lib\site-packages\twisted\internet\defer.py", line 1660, in _inlineCallbacks
    result = current_context.run(gen.send, result)
  File "D:\servers\anaconda3\envs\py3.7\lib\site-packages\scrapy\crawler.py", line 100, in crawl
    self.spider = self._create_spider(*args, **kwargs)
  File "D:\servers\anaconda3\envs\py3.7\lib\site-packages\scrapy\crawler.py", line 112, in _create_spider
    return self.spidercls.from_crawler(self, *args, **kwargs)
  File "D:\servers\anaconda3\envs\py3.7\lib\site-packages\scrapy\spiders\__init__.py", line 48, in from_crawler
    spider = cls(*args, **kwargs)
  File "E:\PycharmProjects\kb_graph_sync\scrapy_modules\spiders\payh_spider.py", line 76, in __init__
    self.init(log_file_name, logger_name)
  File "E:\PycharmProjects\kb_graph_sync\scrapy_modules\spiders\payh_spider.py", line 67, in init
    self.logger = logger
AttributeError: can't set attribute
